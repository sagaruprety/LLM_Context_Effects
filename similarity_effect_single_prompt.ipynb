{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "# import GPUtil\n",
    "import itertools\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from getpass import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# print(GPUtil.showUtilization())\n",
    "# OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-Y2nqeGoOCxTZnITAzuFdT3BlbkFJf6Rm2tmkcssXIov3PMFQ'\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"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\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"LLM_Context_Effects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf_repo_id_mapping = {\n",
    "    'mistral_7B': \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    'llama2_7B':'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'llama2_13B': 'meta-llama/Llama-2-13b-chat-hf',\n",
    "    'llama2_70B': 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    'llama3_8B':'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'llama3_70B': 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "}\n",
    "\n",
    "model_ollama_id_mapping = {\n",
    "    'mistral_7B': \"mistral:7b-instruct-fp16\",\n",
    "    'llama2_7B': 'llama2:7b-chat-fp16',\n",
    "    'llama2_13B': 'llama2:13b-chat-fp16',\n",
    "    'llama2_70B': 'llama2:70b-chat-fp16',\n",
    "    'llama3_8B':'llama3:8b-instruct-fp16',\n",
    "    'llama3_70B': 'llama3:70b-instruct-fp16'\n",
    "}\n",
    "\n",
    "model_name_type_mapping={\n",
    "    'gpt-4o-mini': 'openai',\n",
    "    'gpt-4o': 'openai',\n",
    "    'mistral_7B': 'open-source',\n",
    "    'llama2_7B': 'open-source',\n",
    "    'llama2_13B': 'open-source',\n",
    "    'llama2_70B': 'open-source',\n",
    "    'llama3_8B': 'open-source',\n",
    "    'llama3_70B': 'open-source',\n",
    "}\n",
    "class CustomOutputParser(BaseModel):\n",
    "    score: int = Field(description=\"The similarity score between 0 and 20\")\n",
    "\n",
    "def initialise_openai_models(model_name, temperature):\n",
    "    model = ChatOpenAI(model=model_name, api_key=os.environ[\"OPENAI_API_KEY\"], temperature=temperature, max_tokens=20)\n",
    "    return model.with_structured_output(CustomOutputParser)\n",
    "\n",
    "# def initialise_open_source_models_transformers(model_name, temperature):\n",
    "#     # Use a pipeline as a high-level helper\n",
    "#     repo_id = model_hf_repo_id_mapping[model_name]\n",
    "#     pipe = pipeline(\"text-generation\",\n",
    "#                     model=repo_id,\n",
    "#                     token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "#                     device_map = \"sequential\", max_new_tokens = 10,\n",
    "#                     do_sample = True,\n",
    "#                     return_full_text = False,\n",
    "#                     temperature = temperature,\n",
    "#                     top_k = 50,\n",
    "#                     top_p = 0.9)\n",
    "#     return  HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def initialise_open_source_models_ollama(model_name, temperature):\n",
    "    ollama_id = model_ollama_id_mapping[model_name]\n",
    "    model = ChatOllama(base_url='http://localhost:11434',\n",
    "    model=ollama_id, temperature = temperature, num_predict = 200, format = 'json', num_gpu=-1)\n",
    "    # print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def initialise_models(model_name = 'gpt-4o', model_type = 'openai', temperature= 0.0):\n",
    "    if model_type == 'openai':\n",
    "        return initialise_openai_models(model_name, temperature)\n",
    "    else:\n",
    "        return initialise_open_source_models_ollama(model_name, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.8, 1.0, 1.2, 1.5]\n",
    "similarity_effect_country_pairs = [\n",
    "('U.S.A.', 'Mexico'),\n",
    "('U.S.S.R.', 'Poland'),\n",
    "('China', 'Albania'),\n",
    "('U.S.A.', 'Israel'),\n",
    "('Japan', 'Philippines'),\n",
    "('U.S.A.', 'Canada'),\n",
    "('U.S.S.R.', 'Israel'),\n",
    "('England', 'Ireland'),\n",
    "('Germany', 'Austria'),\n",
    "('U.S.S.R.', 'France'),\n",
    "('Belgium', 'Luxembourg'),\n",
    "('U.S.A.', 'U.S.S.R.'),\n",
    "('China', 'North Korea'),\n",
    "('India', 'Sri Lanka'),\n",
    "('U.S.A.', 'France'),\n",
    "('U.S.S.R.', 'Cuba'),\n",
    "('England', 'Jordan'),\n",
    "('France', 'Israel'),\n",
    "('U.S.A.', 'Germany'),\n",
    "('U.S.S.R.', 'Syria'),\n",
    "('France', 'Algeria')]\n",
    "\n",
    "questions_order_and_1 = {}\n",
    "questions_order_and_2 = {}\n",
    "\n",
    "questions_order_similar_to_1 = {}\n",
    "questions_order_similar_to_2 = {}\n",
    "\n",
    "questions_order_similar_degree_1 = {}\n",
    "questions_order_similar_degree_2 = {}\n",
    "\n",
    "\n",
    "for country1, country2 in similarity_effect_country_pairs:\n",
    "    order_1 = f'{country1}-{country2}'\n",
    "    order_2 = f'{country2}-{country1}'\n",
    "    questions_order_and_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country1} and {country2}? Shape: score: int\"\n",
    "    questions_order_and_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country2} and {country1}? Shape: score: int\"\n",
    "    questions_order_similar_to_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country1} similar to {country2}? Shape: score: int\"\n",
    "    questions_order_similar_to_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country2} similar to {country1}? Shape: score: int\"\n",
    "    questions_order_similar_degree_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country1} similar to {country2}? Shape: score: int\"\n",
    "    questions_order_similar_degree_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country2} similar to {country1}? Shape: score: int\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided. \n",
    "Text: {text}\n",
    "\"\"\"\n",
    "modified_template = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided.\n",
    "                                Text: {text}\n",
    "                                Please provide an integer score.\n",
    "                     \"\"\"\n",
    "template_cot = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided. Also, mention how you arrived at the score.\n",
    "Text: {text}\n",
    "Lets think step by step.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "modified_prompt = ChatPromptTemplate.from_template(modified_template)\n",
    "prompt_cot = ChatPromptTemplate.from_template(template_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ \n",
    "  'gpt-4o-mini',\n",
    "  'llama3_8B',\n",
    "  'llama3_70B',\n",
    " 'gpt-4o',\n",
    "]\n",
    "# models = [ 'gpt-4o', 'gpt-4o-mini', 'llama3_8B']\n",
    "# models = ['llama3_8B']\n",
    "num_trials = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_numeric_output(raw_output, model_type):\n",
    "    if model_type == 'openai':\n",
    "        return raw_output.score\n",
    "    raw_output_str = str(raw_output)\n",
    "    match = re.search(r'\\d+', raw_output_str)\n",
    "    if match:\n",
    "            return int(match.group())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_num</th>\n",
       "      <th>country_pair</th>\n",
       "      <th>model_name</th>\n",
       "      <th>temperature</th>\n",
       "      <th>sim_score_1_to</th>\n",
       "      <th>sim_score_2_to</th>\n",
       "      <th>sim_score_1_and</th>\n",
       "      <th>sim_score_2_and</th>\n",
       "      <th>sim_score_1_degree</th>\n",
       "      <th>sim_score_2_degree</th>\n",
       "      <th>sim_diff_to</th>\n",
       "      <th>sim_diff_and</th>\n",
       "      <th>sim_diff_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [trial_num, country_pair, model_name, temperature, sim_score_1_to, sim_score_2_to, sim_score_1_and, sim_score_2_and, sim_score_1_degree, sim_score_2_degree, sim_diff_to, sim_diff_and, sim_diff_degree]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict_columns = {\n",
    "    'trial_num': [],\n",
    "    'country_pair': '',\n",
    "    'model_name': '',\n",
    "    'temperature': '',\n",
    "    'sim_score_1_to': [],\n",
    "    'sim_score_2_to': [],\n",
    "    'sim_score_1_and': [],\n",
    "    'sim_score_2_and': [],\n",
    "    'sim_score_1_degree': [],\n",
    "    'sim_score_2_degree': [],\n",
    "    'sim_diff_to': [],\n",
    "    'sim_diff_and': [],\n",
    "    'sim_diff_degree': []\n",
    "}\n",
    "# Define the file path\n",
    "file_path = './results_sampling_exp/results_single_prompt.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    # Create an empty DataFrame from the dictionary\n",
    "    df = pd.DataFrame(columns=results_dict_columns)\n",
    "\n",
    "# Print the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(prompt, model, order, model_name, model_type, temperature, prompt_style):\n",
    "    if model_type == 'openai':\n",
    "        prompt_chain = prompt | model\n",
    "    else:\n",
    "        prompt_chain = prompt | model | JsonOutputParser()\n",
    "    return (prompt_chain).with_config({\n",
    "\"metadata\": {\n",
    "    'country-pair-order': order,\n",
    "    'model_name':model_name,\n",
    "    'temperature': temperature,\n",
    "    'prompt_style': prompt_style\n",
    "}}\n",
    ")\n",
    "\n",
    "def get_output(prompt, model, order, model_name, temperature, ques, model_type, prompt_style):\n",
    "    chain = create_chain(prompt, model, order, model_name, model_type, temperature, prompt_style)\n",
    "    return chain.invoke({\"text\": ques})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6468\n"
     ]
    }
   ],
   "source": [
    "experimental_combinations = list(itertools.product(\n",
    "    ['llama3_8B'],\n",
    "    temperatures,\n",
    "    similarity_effect_country_pairs,\n",
    "    range(1, num_trials + 1)\n",
    "))\n",
    "print(len(experimental_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save progress\n",
    "def save_progress(index):\n",
    "    with open('experiment_progress.json', 'w') as f:\n",
    "        json.dump({'last_completed_index': index}, f)\n",
    "\n",
    "# Function to load progress\n",
    "def load_progress():\n",
    "    try:\n",
    "        with open('experiment_progress.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get('last_completed_index', -1)\n",
    "    except FileNotFoundError:\n",
    "        return -1\n",
    "\n",
    "# Load the last completed index\n",
    "start_index = load_progress() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combination 2/6468: Model: llama3_8B, Temperature: 0.8, Countries: U.S.A.-Mexico, Trial: 2\n",
      "model='llama3:8b-instruct-fp16' num_gpu=-1 num_predict=200 temperature=0.8 format='json' base_url='http://localhost:11434'\n",
      "Model deleted.\n",
      "Experiment completed or interrupted. You can resume from the last saved point.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i, (model_name, temperature, (country1, country2), trial_num) in enumerate(experimental_combinations[start_index:2], start=start_index):\n",
    "        model_type = model_name_type_mapping[model_name]\n",
    "        model = initialise_models(model_name, model_type, temperature)\n",
    "\n",
    "        order_1 = f'{country1}-{country2}'\n",
    "        order_2 = f'{country2}-{country1}'\n",
    "\n",
    "        print(f'Processing combination {i+1}/{len(experimental_combinations)}: '\n",
    "              f'Model: {model_name}, Temperature: {temperature}, '\n",
    "              f'Countries: {order_1}, Trial: {trial_num}')\n",
    "\n",
    "        ques_1_to = questions_order_similar_to_1[order_1]\n",
    "        ques_2_to = questions_order_similar_to_2[order_2]\n",
    "        ques_1_and = questions_order_and_1[order_1]\n",
    "        ques_2_and = questions_order_and_2[order_2]\n",
    "        ques_1_degree = questions_order_similar_degree_1[order_1]\n",
    "        ques_2_degree = questions_order_similar_degree_2[order_2]\n",
    "\n",
    "\n",
    "        output_1_to = get_output(prompt, model, order_1, model_name, temperature, ques_1_to, model_type, 'sst')\n",
    "        output_2_to = get_output(prompt, model, order_2, model_name, temperature, ques_2_to, model_type, 'sst')\n",
    "        sim_score_1_to = parse_numeric_output(output_1_to, model_type)\n",
    "        sim_score_2_to = parse_numeric_output(output_2_to, model_type)\n",
    "\n",
    "        output_1_and = get_output(prompt, model, order_1, model_name, temperature, ques_1_and, model_type, 'ssa')\n",
    "        output_2_and = get_output(prompt, model, order_2, model_name, temperature, ques_2_and, model_type, 'ssa')\n",
    "        sim_score_1_and = parse_numeric_output(output_1_and, model_type)\n",
    "        sim_score_2_and = parse_numeric_output(output_2_and, model_type)\n",
    "\n",
    "        output_1_degree = get_output(prompt, model, order_1, model_name, temperature, ques_1_degree, model_type, 'ssd')\n",
    "        output_2_degree = get_output(prompt, model, order_2, model_name, temperature, ques_2_degree, model_type, 'ssd')\n",
    "        sim_score_1_degree = parse_numeric_output(output_1_degree, model_type)\n",
    "        sim_score_2_degree = parse_numeric_output(output_2_degree, model_type)            \n",
    "\n",
    "        if sim_score_1_to!=None and sim_score_2_to!=None:\n",
    "                sim_diff_to = sim_score_1_to - sim_score_2_to\n",
    "        else:\n",
    "            print(f' cannot parse output {output_1_to} or {output_2_to} for Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}, Trial: {trial_num}, exp_index: {i}')\n",
    "            sim_diff_to = None\n",
    "\n",
    "        if sim_score_1_and!=None and sim_score_2_and!=None:\n",
    "            sim_diff_and = sim_score_1_and - sim_score_2_and\n",
    "        else:\n",
    "            print(f' cannot parse output {output_1_and} or {output_2_and} for Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}, Trial: {trial_num}, exp_index: {i}')\n",
    "            sim_diff_and = None\n",
    "\n",
    "        if sim_score_1_degree!=None and sim_score_2_degree!=None:\n",
    "            sim_diff_degree = sim_score_1_degree - sim_score_2_degree\n",
    "        else:   \n",
    "            print(f' cannot parse output {output_1_degree} or {output_2_degree} for Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}, Trial: {trial_num}, exp_index: {i}')\n",
    "            sim_diff_degree = None\n",
    "\n",
    "        results_dict_columns['temperature'] = temperature\n",
    "\n",
    "        results_dict_columns['model_name'] = model_name\n",
    "        results_dict_columns['country_pair'] = order_1\n",
    "        results_dict_columns['sim_score_1_to'] = sim_score_1_to\n",
    "        results_dict_columns['sim_score_2_to'] = sim_score_2_to\n",
    "        results_dict_columns['sim_score_1_and'] = sim_score_1_and\n",
    "        results_dict_columns['sim_score_2_and'] = sim_score_2_and\n",
    "        results_dict_columns['sim_score_1_degree'] = sim_score_1_degree\n",
    "        results_dict_columns['sim_score_2_degree'] = sim_score_2_degree\n",
    "        results_dict_columns['sim_diff_to'] = sim_diff_to\n",
    "        results_dict_columns['sim_diff_and'] = sim_diff_and\n",
    "        results_dict_columns['sim_diff_degree'] = sim_diff_degree\n",
    "        results_dict_columns['trial_num'] = trial_num\n",
    "        # create a dataframe from the results_dict_columns\n",
    "        df = pd.DataFrame.from_dict([results_dict_columns])\n",
    "        \n",
    "        # Write to CSV file\n",
    "        if not os.path.exists(file_path):\n",
    "            df.to_csv(file_path, index=False, mode='w')\n",
    "        else:\n",
    "            df.to_csv(file_path, index=False, mode='a', header=False)\n",
    "        save_progress(i)\n",
    "        \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nExperiment interrupted. Progress saved.\")\n",
    "\n",
    "finally:\n",
    "    # Clean up resources\n",
    "    if 'model' in locals():\n",
    "        print(model)\n",
    "        del model\n",
    "        print('Model deleted.')\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Experiment completed or interrupted. You can resume from the last saved point.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
