{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "# import GPUtil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama, HuggingFaceHub, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# print(GPUtil.showUtilization())\n",
    "# OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-Y2nqeGoOCxTZnITAzuFdT3BlbkFJf6Rm2tmkcssXIov3PMFQ'\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"ls__93636df794f14ccba7162354d46779d8\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"LLM_Context_Effects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf_repo_id_mapping = {\n",
    "    'mistral_7B': \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    'llama2_7B':'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'llama2_13B': 'meta-llama/Llama-2-13b-chat-hf',\n",
    "    'llama2_70B': 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    'llama3_8B':'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'llama3_70B': 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "}\n",
    "\n",
    "model_ollama_id_mapping = {\n",
    "    'mistral_7B': \"mistral:7b-instruct-fp16\",\n",
    "    'llama2_7B': 'llama2:7b-chat-fp16',\n",
    "    'llama2_13B': 'llama2:13b-chat-fp16',\n",
    "    'llama2_70B': 'llama2:70b-chat-fp16',\n",
    "    'llama3_8B':'llama3:8b-instruct-fp16',\n",
    "    'llama3_70B': 'llama3:70b-instruct-fp16'\n",
    "}\n",
    "\n",
    "model_name_type_mapping={\n",
    "    'gpt-3.5-turbo': 'openai',\n",
    "    'gpt-4': 'openai',\n",
    "    'mistral_7B': 'open-source',\n",
    "    'llama2_7B': 'open-source',\n",
    "    'llama2_13B': 'open-source',\n",
    "    'llama2_70B': 'open-source',\n",
    "    'llama3_8B': 'open-source',\n",
    "    'llama3_70B': 'open-source',\n",
    "}\n",
    "\n",
    "def initialise_openai_models(model_name, temperature):\n",
    "    model = ChatOpenAI(model=model_name, api_key=os.environ[\"OPENAI_API_KEY\"], temperature=temperature, max_tokens=200)\n",
    "    return model\n",
    "\n",
    "def initialise_open_source_models_transformers(model_name, temperature):\n",
    "    # Use a pipeline as a high-level helper\n",
    "    repo_id = model_hf_repo_id_mapping[model_name]\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                    model=repo_id,\n",
    "                    token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "                    device_map = \"sequential\", max_new_tokens = 10,\n",
    "                    do_sample = True,\n",
    "                    return_full_text = False,\n",
    "                    temperature = temperature,\n",
    "                    top_k = 50,\n",
    "                    top_p = 0.9)\n",
    "    return  HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def initialise_open_source_models_ollama(model_name, temperature):\n",
    "    ollama_id = model_ollama_id_mapping[model_name]\n",
    "    model = Ollama(base_url='http://localhost:11434',\n",
    "    model=ollama_id, temperature = temperature, num_predict = 200, format = 'json', num_gpu=-1)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def initialise_models(model_name = 'mistral_7B', model_type = 'openai', temperature= 0.0):\n",
    "    if model_type == 'openai':\n",
    "        return initialise_openai_models(model_name, temperature)\n",
    "    else:\n",
    "        return initialise_open_source_models_ollama(model_name, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U.S.A.-Mexico': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Mexico? Shape: score: int', 'U.S.S.R.-Poland': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Poland? Shape: score: int', 'China-Albania': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is China similar to Albania? Shape: score: int', 'U.S.A.-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Israel? Shape: score: int', 'Japan-Philippines': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is Japan similar to Philippines? Shape: score: int', 'U.S.A.-Canada': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Canada? Shape: score: int', 'U.S.S.R.-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Israel? Shape: score: int', 'England-Ireland': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is England similar to Ireland? Shape: score: int', 'Germany-Austria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is Germany similar to Austria? Shape: score: int', 'U.S.S.R.-France': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to France? Shape: score: int', 'Belgium-Luxembourg': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is Belgium similar to Luxembourg? Shape: score: int', 'U.S.A.-U.S.S.R.': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to U.S.S.R.? Shape: score: int', 'China-North Korea': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is China similar to North Korea? Shape: score: int', 'India-Sri Lanka': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is India similar to Sri Lanka? Shape: score: int', 'U.S.A.-France': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to France? Shape: score: int', 'U.S.S.R.-Cuba': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Cuba? Shape: score: int', 'England-Jordan': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is England similar to Jordan? Shape: score: int', 'France-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is France similar to Israel? Shape: score: int', 'U.S.A.-Germany': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Germany? Shape: score: int', 'U.S.S.R.-Syria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Syria? Shape: score: int', 'France-Algeria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is France similar to Algeria? Shape: score: int'}\n"
     ]
    }
   ],
   "source": [
    "temperatures = [0.001, 0.5, 1.0, 1.5]\n",
    "similarity_effect_country_pairs = [\n",
    "('U.S.A.', 'Mexico'),\n",
    "('U.S.S.R.', 'Poland'),\n",
    "('China', 'Albania'),\n",
    "('U.S.A.', 'Israel'),\n",
    "('Japan', 'Philippines'),\n",
    "('U.S.A.', 'Canada'),\n",
    "('U.S.S.R.', 'Israel'),\n",
    "('England', 'Ireland'),\n",
    "('Germany', 'Austria'),\n",
    "('U.S.S.R.', 'France'),\n",
    "('Belgium', 'Luxembourg'),\n",
    "('U.S.A.', 'U.S.S.R.'),\n",
    "('China', 'North Korea'),\n",
    "('India', 'Sri Lanka'),\n",
    "('U.S.A.', 'France'),\n",
    "('U.S.S.R.', 'Cuba'),\n",
    "('England', 'Jordan'),\n",
    "('France', 'Israel'),\n",
    "('U.S.A.', 'Germany'),\n",
    "('U.S.S.R.', 'Syria'),\n",
    "('France', 'Algeria')]\n",
    "\n",
    "questions_order_1 = {}\n",
    "questions_order_2 = {}\n",
    "\n",
    "questions_order_similar_to_1 = {}\n",
    "questions_order_similar_to_2 = {}\n",
    "\n",
    "questions_order_similar_degree_1 = {}\n",
    "questions_order_similar_degree_2 = {}\n",
    "\n",
    "\n",
    "for country1, country2 in similarity_effect_country_pairs:\n",
    "    order_1 = f'{country1}-{country2}'\n",
    "    order_2 = f'{country2}-{country1}'\n",
    "    questions_order_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country1} and {country2}? Shape: score: int\"\n",
    "    questions_order_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country2} and {country1}? Shape: score: int\"\n",
    "    questions_order_similar_to_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country1} similar to {country2}? Shape: score: int\"\n",
    "    questions_order_similar_to_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country2} similar to {country1}? Shape: score: int\"\n",
    "\n",
    "    questions_order_similar_degree_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country1} similar to {country2}? Shape: score: int\"\n",
    "    questions_order_similar_degree_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country2} similar to {country1}? Shape: score: int\"\n",
    "\n",
    "print(questions_order_similar_to_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided. \n",
    "Text: {text}\n",
    "\"\"\"\n",
    "modified_template = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided.\n",
    "                                Text: {text}\n",
    "                                Please provide an integer score.\n",
    "                     \"\"\"\n",
    "template_cot = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided. Also, mention how you arrived at the score.\n",
    "Text: {text}\n",
    "Lets think step by step.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "modified_prompt = ChatPromptTemplate.from_template(modified_template)\n",
    "prompt_cot = ChatPromptTemplate.from_template(template_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ \n",
    "    'mistral_7B',\n",
    " 'llama2_7B',\n",
    "  'llama3_8B',\n",
    " 'llama2_13B',\n",
    "  'gpt-3.5-turbo',\n",
    " 'gpt-4'\n",
    "]\n",
    "models = [ 'llama2_70B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_numeric_output(raw_output):\n",
    "    match = re.search(r'\\d+', raw_output)\n",
    "    # print(match, match.group())\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_columns = {\n",
    "    'country_pair': '',\n",
    "    'prompt_style': 'single',\n",
    "    'model_name': '',\n",
    "    'temperature': '',\n",
    "    'sim_score_1': [],\n",
    "    'sim_score_2': [],\n",
    "    'sim_diff': [],\n",
    "    'p-values': []\n",
    "}\n",
    "# Define the file path\n",
    "file_path = './results/results_single_prompt_similar_degree.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    # Create an empty DataFrame from the dictionary\n",
    "    df = pd.DataFrame(columns=results_dict_columns)\n",
    "\n",
    "# Print the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(prompt, model, order, model_name, temperature):\n",
    "    return (prompt | model | StrOutputParser()).with_config({\n",
    "\"metadata\": {\n",
    "    'country-pair-order': order,\n",
    "    'model_name':model_name,\n",
    "    'temperature': temperature,\n",
    "}}\n",
    ")\n",
    "\n",
    "def get_output(prompt, model, order, model_name, temperature, ques):\n",
    "    chain = create_chain(prompt, model, order, model_name, temperature)\n",
    "    return chain.invoke({\"text\": ques})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 200, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 0.001, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 200, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 0.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 0.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\":12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 0.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 0.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 0.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 0.5, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 0.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 0.5, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\":12   \n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 0.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 0.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\":10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 0.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 0.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 0.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 0.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 0.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 200, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 1.0, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 1.0, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 1.0, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 1.0, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 1.0, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 1.0, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 1.0, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 1.0, output1: {\n",
      "\"score\": 17\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 1.0, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 1.0, output1: {\n",
      "\"score\": 17\n",
      "}, output2: {\n",
      "\"score\":15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 1.0, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 1.0, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 1.0, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\"score\":14}\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 200, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 1.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 1.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 1.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 1.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 1.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 1.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 1.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 1.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 1.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 1.5, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 1.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\":12   \n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 1.5, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 1.5, output1: {\n",
      "\"score\":12   \n",
      "}, output2: {\n",
      "\"score\": 5\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 1.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 1.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 1.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 1.5, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 1.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 1.5, output1: {\n",
      "\"score\":10\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 1.5, output1: {\n",
      "\"score\": 12\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 1.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 12\n",
      "}\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 1.5, output1: {\n",
      "\"score\":12\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name in models:\n",
    "    model_type = model_name_type_mapping[model_name]\n",
    "    results_dict_columns['model_name'] = model_name\n",
    "    for temperature in temperatures:\n",
    "        model = initialise_models(model_name, model_type, temperature)\n",
    "        results_dict_columns['temperature'] = temperature\n",
    "        for order_1, order_2 in zip(questions_order_similar_to_1, questions_order_similar_to_2):\n",
    "            results_dict_columns['country_pair'] = order_1\n",
    "            ques_1 = questions_order_similar_degree_1[order_1]\n",
    "            ques_2 = questions_order_similar_degree_2[order_2]\n",
    "            output_1 = get_output(prompt, model, order_1, model_name, temperature, ques_1)\n",
    "            output_2 = get_output(prompt, model, order_2, model_name, temperature, ques_2)\n",
    "            parsed_output_1 = parse_numeric_output(output_1)\n",
    "            if  parsed_output_1:\n",
    "                    sim_score_1 = int(parsed_output_1)\n",
    "            else:\n",
    "                output_1 = get_output(modified_prompt, model, order_1, model_name, temperature, ques_1)\n",
    "                parsed_output_1 = parse_numeric_output(output_1)\n",
    "                if  parsed_output_1:\n",
    "                    sim_score_1 = int(parsed_output_1)\n",
    "                else:\n",
    "                    sim_score_1 = None\n",
    "                    print(f' cannot parse output {output_1} for Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}')\n",
    "            parsed_output_2 = parse_numeric_output(output_2)\n",
    "            if parsed_output_2:\n",
    "                sim_score_2 = int(parsed_output_2)\n",
    "                \n",
    "            else:\n",
    "                output_2 = get_output(modified_prompt, model, order_2, model_name, temperature, ques_2)\n",
    "                parsed_output_2 = parse_numeric_output(output_2)\n",
    "                if  parsed_output_2:\n",
    "                    sim_score_2 = int(parsed_output_2)\n",
    "                else:\n",
    "                    sim_score_2 = None\n",
    "                    print(f' cannot parse output {output_2} for Model_name: {model_name}, Pair: {order_2}, Temperature: {temperature}')\n",
    "            if sim_score_1!=None and sim_score_2!=None:\n",
    "                sim_diff = sim_score_1 - sim_score_2\n",
    "            else:\n",
    "                sim_diff = None\n",
    "            print(f'for Model_name: {model_name}, Pair: {order_2}, Temperature: {temperature}, output1: {output_1}, output2: {output_2}')\n",
    "            results_dict_columns['sim_score_1'] = sim_score_1\n",
    "            results_dict_columns['sim_score_2'] = sim_score_2\n",
    "            results_dict_columns['sim_diff'] = sim_diff\n",
    "            df = pd.concat([df, pd.DataFrame.from_dict([results_dict_columns])])\n",
    "            # del model\n",
    "        # print('model deleted..')\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_pair</th>\n",
       "      <th>prompt_style</th>\n",
       "      <th>model_name</th>\n",
       "      <th>temperature</th>\n",
       "      <th>sim_score_1</th>\n",
       "      <th>sim_score_2</th>\n",
       "      <th>sim_diff</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Mexico</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>-2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S.S.R.-Poland</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China-Albania</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S.A.-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>-2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan-Philippines</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.S.A.-Canada</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U.S.S.R.-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>England-Ireland</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany-Austria</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S.S.R.-France</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Belgium-Luxembourg</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>U.S.A.-U.S.S.R.</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>China-North Korea</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>India-Sri Lanka</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>U.S.A.-France</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>U.S.S.R.-Cuba</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>England-Jordan</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>France-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>-6</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>U.S.A.-Germany</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>-2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>U.S.S.R.-Syria</td>\n",
       "      <td>single</td>\n",
       "      <td>llama3_70B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          country_pair prompt_style  model_name  temperature  sim_score_1  \\\n",
       "0        U.S.A.-Mexico       single  llama3_70B        0.001           12   \n",
       "1      U.S.S.R.-Poland       single  llama3_70B        0.001            6   \n",
       "2        China-Albania       single  llama3_70B        0.001            4   \n",
       "3        U.S.A.-Israel       single  llama3_70B        0.001           12   \n",
       "4    Japan-Philippines       single  llama3_70B        0.001           12   \n",
       "5        U.S.A.-Canada       single  llama3_70B        0.001           16   \n",
       "6      U.S.S.R.-Israel       single  llama3_70B        0.001            4   \n",
       "7      England-Ireland       single  llama3_70B        0.001           14   \n",
       "8      Germany-Austria       single  llama3_70B        0.001           16   \n",
       "9      U.S.S.R.-France       single  llama3_70B        0.001            6   \n",
       "10  Belgium-Luxembourg       single  llama3_70B        0.001           17   \n",
       "11     U.S.A.-U.S.S.R.       single  llama3_70B        0.001            6   \n",
       "12   China-North Korea       single  llama3_70B        0.001           12   \n",
       "13     India-Sri Lanka       single  llama3_70B        0.001           14   \n",
       "14       U.S.A.-France       single  llama3_70B        0.001           12   \n",
       "15       U.S.S.R.-Cuba       single  llama3_70B        0.001           14   \n",
       "16      England-Jordan       single  llama3_70B        0.001            4   \n",
       "17       France-Israel       single  llama3_70B        0.001            6   \n",
       "18      U.S.A.-Germany       single  llama3_70B        0.001           12   \n",
       "19      U.S.S.R.-Syria       single  llama3_70B        0.001            4   \n",
       "\n",
       "    sim_score_2  sim_diff p-values  \n",
       "0            14        -2       []  \n",
       "1             4         2       []  \n",
       "2             4         0       []  \n",
       "3            14        -2       []  \n",
       "4            12         0       []  \n",
       "5            16         0       []  \n",
       "6             4         0       []  \n",
       "7            14         0       []  \n",
       "8            16         0       []  \n",
       "9             4         2       []  \n",
       "10           16         1       []  \n",
       "11            6         0       []  \n",
       "12           12         0       []  \n",
       "13           14         0       []  \n",
       "14           12         0       []  \n",
       "15           12         2       []  \n",
       "16            6        -2       []  \n",
       "17           12        -6       []  \n",
       "18           14        -2       []  \n",
       "19            6        -2       []  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_path, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model deleted..\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "print('model deleted..')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
