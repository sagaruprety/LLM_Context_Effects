{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "# import GPUtil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama, HuggingFaceHub, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# print(GPUtil.showUtilization())\n",
    "# OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-Y2nqeGoOCxTZnITAzuFdT3BlbkFJf6Rm2tmkcssXIov3PMFQ'\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"ls__93636df794f14ccba7162354d46779d8\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"LLM_Context_Effects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf_repo_id_mapping = {\n",
    "    'mistral_7B': \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    'llama2_7B':'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'llama2_13B': 'meta-llama/Llama-2-13b-chat-hf',\n",
    "    'llama2_70B': 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    'llama3_8B':'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'llama3_70B': 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "}\n",
    "\n",
    "model_ollama_id_mapping = {\n",
    "    'mistral_7B': \"mistral:7b-instruct-fp16\",\n",
    "    'llama2_7B': 'llama2:7b-chat-fp16',\n",
    "    'llama2_13B': 'llama2:13b-chat-fp16',\n",
    "    'llama2_70B': 'llama2:70b-chat-fp16',\n",
    "    'llama3_8B':'llama3:8b-instruct-fp16',\n",
    "    'llama3_70B': 'llama3:70b-instruct-fp16'\n",
    "}\n",
    "\n",
    "model_name_type_mapping={\n",
    "    'gpt-3.5-turbo': 'openai',\n",
    "    'gpt-4': 'openai',\n",
    "    'mistral_7B': 'open-source',\n",
    "    'llama2_7B': 'open-source',\n",
    "    'llama2_13B': 'open-source',\n",
    "    'llama2_70B': 'open-source',\n",
    "    'llama3_8B': 'open-source',\n",
    "    'llama3_70B': 'open-source',\n",
    "}\n",
    "\n",
    "def initialise_openai_models(model_name, temperature):\n",
    "    model = ChatOpenAI(model=model_name, api_key=os.environ[\"OPENAI_API_KEY\"], temperature=temperature, max_tokens=10)\n",
    "    return model\n",
    "\n",
    "def initialise_open_source_models_transformers(model_name, temperature):\n",
    "    # Use a pipeline as a high-level helper\n",
    "    repo_id = model_hf_repo_id_mapping[model_name]\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                    model=repo_id,\n",
    "                    token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "                    device_map = \"sequential\", max_new_tokens = 10,\n",
    "                    do_sample = True,\n",
    "                    return_full_text = False,\n",
    "                    temperature = temperature,\n",
    "                    top_k = 50,\n",
    "                    top_p = 0.9)\n",
    "    return  HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def initialise_open_source_models_ollama(model_name, temperature):\n",
    "    ollama_id = model_ollama_id_mapping[model_name]\n",
    "    model = Ollama(base_url='http://localhost:11434',\n",
    "    model=ollama_id, temperature = temperature, num_predict = 10, format = 'json', num_gpu=-1)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def initialise_models(model_name = 'mistral_7B', model_type = 'openai', temperature= 0.0):\n",
    "    if model_type == 'openai':\n",
    "        return initialise_openai_models(model_name, temperature)\n",
    "    else:\n",
    "        return initialise_open_source_models_ollama(model_name, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.001, 0.5, 1.0, 1.5]\n",
    "similarity_effect_country_pairs = [\n",
    "('U.S.A.', 'Mexico'),\n",
    "('U.S.S.R.', 'Poland'),\n",
    "('China', 'Albania'),\n",
    "('U.S.A.', 'Israel'),\n",
    "('Japan', 'Philippines'),\n",
    "('U.S.A.', 'Canada'),\n",
    "('U.S.S.R.', 'Israel'),\n",
    "('England', 'Ireland'),\n",
    "('Germany', 'Austria'),\n",
    "('U.S.S.R.', 'France'),\n",
    "('Belgium', 'Luxembourg'),\n",
    "('U.S.A.', 'U.S.S.R.'),\n",
    "('China', 'North Korea'),\n",
    "('India', 'Sri Lanka'),\n",
    "('U.S.A.', 'France'),\n",
    "('U.S.S.R.', 'Cuba'),\n",
    "('England', 'Jordan'),\n",
    "('France', 'Israel'),\n",
    "('U.S.A.', 'Germany'),\n",
    "('U.S.S.R.', 'Syria'),\n",
    "('France', 'Algeria')]\n",
    "\n",
    "questions_order_1 = {}\n",
    "questions_order_2 = {}\n",
    "\n",
    "for country1, country2 in similarity_effect_country_pairs:\n",
    "    order_1 = f'{country1}-{country2}'\n",
    "    order_2 = f'{country2}-{country1}'\n",
    "    questions_order_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country1} and {country2}? Shape: score: int\"\n",
    "    questions_order_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country2} and {country1}? Shape: score: int\"\n",
    "\n",
    "# print(questions_order_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided.\n",
    "Text: {text}\n",
    "\"\"\"\n",
    "modified_template = \"\"\"Answer the following question to the best of your knowledge. Your answer should be a json of shape provided.\n",
    "                                Text: {text}\n",
    "                                Please provide an integer score.\n",
    "                     \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "modified_prompt = ChatPromptTemplate.from_template(modified_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'mistral_7B',\n",
    " 'llama2_7B',\n",
    "  'llama3_8B',\n",
    " 'llama2_13B',\n",
    " 'llama2_70B',\n",
    " 'gpt-3.5-turbo',\n",
    " 'gpt-4']\n",
    "# models = [ 'mistral_7B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_numeric_output(raw_output):\n",
    "    match = re.search(r'\\d+', raw_output)\n",
    "    # print(match, match.group())\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_columns = {\n",
    "    'country_pair': '',\n",
    "    'prompt_style': 'single',\n",
    "    'model_name': '',\n",
    "    'temperature': '',\n",
    "    'sim_score_1': [],\n",
    "    'sim_score_2': [],\n",
    "    'sim_diff': [],\n",
    "    'p-values': []\n",
    "}\n",
    "# Define the file path\n",
    "file_path = './results/results_variability.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    # Create an empty DataFrame from the dictionary\n",
    "    df = pd.DataFrame(columns=results_dict_columns)\n",
    "\n",
    "# Print the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(prompt, model, order, model_name, temperature):\n",
    "    return prompt | model | StrOutputParser().with_config({\n",
    "\"metadata\": {\n",
    "    'country-pair-order': order,\n",
    "    'model_name':model_name,\n",
    "    'temperature': temperature,\n",
    "}}\n",
    ")\n",
    "\n",
    "def get_output(prompt, model, order, model_name, temperature, ques):\n",
    "    chain = create_chain(prompt, model, order, model_name, temperature)\n",
    "    return chain.invoke({\"text\": ques})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Mexico-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Mexico-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Poland-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Poland-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Albania-China, Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Albania-China, Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Israel-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Israel-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Philippines-Japan, Temperature: 0.001, output1: {\n",
      "\"score\": 16\n",
      "}, output2: {\n",
      "\"score\": 16\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Philippines-Japan, Temperature: 0.001, output1: {\n",
      "\"score\": 16\n",
      "}, output2: {\n",
      "\"score\": 16\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Canada-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Canada-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Israel-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Israel-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Ireland-England, Temperature: 0.001, output1: {\n",
      "\"score\": 19\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Ireland-England, Temperature: 0.001, output1: {\n",
      "\"score\": 19\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Austria-Germany, Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Austria-Germany, Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: France-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: France-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Luxembourg-Belgium, Temperature: 0.001, output1: {\n",
      "\"score\": 19\n",
      "}, output2: {\n",
      "\"score\": 19\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Luxembourg-Belgium, Temperature: 0.001, output1: {\n",
      "\"score\": 19\n",
      "}, output2: {\n",
      "\"score\": 19\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: U.S.S.R.-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: U.S.S.R.-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 10\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: North Korea-China, Temperature: 0.001, output1: {\n",
      "\"score\": 17\n",
      "}, output2: {\n",
      "\"score\": 17\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: North Korea-China, Temperature: 0.001, output1: {\n",
      "\"score\": 17\n",
      "}, output2: {\n",
      "\"score\": 17\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Sri Lanka-India, Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Sri Lanka-India, Temperature: 0.001, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: France-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: France-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Cuba-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Cuba-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Jordan-England, Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Jordan-England, Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Israel-France, Temperature: 0.001, output1: {\n",
      "\"score\": 17\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Israel-France, Temperature: 0.001, output1: {\n",
      "\"score\": 17\n",
      "}, output2: {\n",
      "\"score\": 15\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Germany-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 16\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Germany-U.S.A., Temperature: 0.001, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 16\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Syria-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 1\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Syria-U.S.S.R., Temperature: 0.001, output1: {\n",
      "\"score\": 1\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Algeria-France, Temperature: 0.001, output1: {\n",
      "\"score\": 16\n",
      "}, output2: {\n",
      "\"score\": 16\n",
      "}\n",
      "Model - mistral_7B, temp - 0.001 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Algeria-France, Temperature: 0.001, output1: {\n",
      "\"score\": 16\n",
      "}, output2: {\n",
      "\"score\": 16\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Mexico-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Mexico-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\": 7\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Poland-U.S.S.R., Temperature: 0.5, output1: {\n",
      "\"score\": 7\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Poland-U.S.S.R., Temperature: 0.5, output1: {\n",
      "\"score\": 15\n",
      "}, output2: {\n",
      "\"score\": 10\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Albania-China, Temperature: 0.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 4\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Albania-China, Temperature: 0.5, output1: {\n",
      "\"score\": 5\n",
      "}, output2: {\n",
      "\"score\": 1\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 1\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "for Model_name: mistral_7B, Pair: Israel-U.S.A., Temperature: 0.5, output1: {\n",
      "\"score\": 18\n",
      "}, output2: {\n",
      "\"score\": 18\n",
      "}\n",
      "Model - mistral_7B, temp - 0.5 Iteration No. 2\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral:7b-instruct-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 10, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m chain_1 \u001b[38;5;241m=\u001b[39m create_chain(prompt, model, order_1, model_name, temperature)    \n\u001b[1;32m     17\u001b[0m chain_2 \u001b[38;5;241m=\u001b[39m create_chain(prompt, model, order_1, model_name, temperature)\n\u001b[0;32m---> 18\u001b[0m output_1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mques_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m output_2 \u001b[38;5;241m=\u001b[39m get_output(prompt, model, order_2, model_name, temperature, ques_2)\n\u001b[1;32m     20\u001b[0m parsed_output_1 \u001b[38;5;241m=\u001b[39m parse_numeric_output(output_1)\n",
      "Cell \u001b[0;32mIn[69], line 12\u001b[0m, in \u001b[0;36mget_output\u001b[0;34m(prompt, model, order, model_name, temperature, ques)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_output\u001b[39m(prompt, model, order, model_name, temperature, ques):\n\u001b[1;32m     11\u001b[0m     chain \u001b[38;5;241m=\u001b[39m create_chain(prompt, model, order, model_name, temperature)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mques\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_core/language_models/llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    288\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_community/llms/ollama.py:417\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 417\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_community/llms/ollama.py:326\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    319\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    325\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    328\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_community/llms/ollama.py:172\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/langchain_community/llms/ollama.py:231\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     request_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, []),\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    229\u001b[0m     }\n\u001b[0;32m--> 231\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM_Context_Effects/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name in models:\n",
    "    model_type = model_name_type_mapping[model_name]\n",
    "    results_dict_columns['model_name'] = model_name\n",
    "    for temperature in temperatures:\n",
    "        model = initialise_models(model_name, model_type, temperature)\n",
    "        results_dict_columns['temperature'] = temperature\n",
    "        for order_1, order_2 in zip(questions_order_1, questions_order_2):\n",
    "            sim_score_1_list = []\n",
    "            sim_score_2_list = []\n",
    "            sim_diff_list = []\n",
    "            results_dict_columns['country_pair'] = order_1\n",
    "            for i in range(10):\n",
    "                print(f'Model - {model_name}, temp - {temperature} Iteration No. {i+1}')\n",
    "                ques_1 = questions_order_1[order_1]\n",
    "                ques_2 = questions_order_2[order_2]\n",
    "                chain_1 = create_chain(prompt, model, order_1, model_name, temperature)    \n",
    "                chain_2 = create_chain(prompt, model, order_1, model_name, temperature)\n",
    "                output_1 = get_output(prompt, model, order_1, model_name, temperature, ques_1)\n",
    "                output_2 = get_output(prompt, model, order_2, model_name, temperature, ques_2)\n",
    "                parsed_output_1 = parse_numeric_output(output_1)\n",
    "                if  parsed_output_1:\n",
    "                        sim_score_1 = int(parsed_output_1)\n",
    "                else:\n",
    "                    output_1 = get_output(modified_prompt, model, order_1, model_name, temperature, ques_1)\n",
    "                    parsed_output_1 = parse_numeric_output(output_1)\n",
    "                    if  parsed_output_1:\n",
    "                        sim_score_1 = int(parsed_output_1)\n",
    "                    else:\n",
    "                        sim_score_1 = None\n",
    "                        print(f' cannot parse output {output_1} for Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}')\n",
    "                sim_score_1_list.append(sim_score_1)\n",
    "                parsed_output_2 = parse_numeric_output(output_2)\n",
    "                if parsed_output_2:\n",
    "                    sim_score_2 = int(parsed_output_2)\n",
    "                    \n",
    "                else:\n",
    "                    output_2 = get_output(modified_prompt, model, order_2, model_name, temperature, ques_2)\n",
    "                    parsed_output_2 = parse_numeric_output(output_2)\n",
    "                    if  parsed_output_2:\n",
    "                        sim_score_2 = int(parsed_output_2)\n",
    "                    else:\n",
    "                        sim_score_2 = None\n",
    "                        print(f' cannot parse output {output_2} for Model_name: {model_name}, Pair: {order_2}, Temperature: {temperature}')\n",
    "                sim_score_2_list.append(sim_score_2)\n",
    "                if sim_score_1!=None and sim_score_2!=None:\n",
    "                    sim_diff_list.append(sim_score_1 - sim_score_2)\n",
    "                else:\n",
    "                    sim_diff_list.append(None)\n",
    "                print(f'for Model_name: {model_name}, Pair: {order_2}, Temperature: {temperature}, output1: {output_1}, output2: {output_2}')\n",
    "            results_dict_columns['sim_score_1'] = sim_score_1_list\n",
    "            results_dict_columns['sim_score_2'] = sim_score_2_list\n",
    "            results_dict_columns['sim_diff'] = sim_diff_list\n",
    "            df = pd.concat([df, pd.DataFrame.from_dict([results_dict_columns])])\n",
    "            # del model\n",
    "        # print('model deleted..')\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_pair</th>\n",
       "      <th>prompt_style</th>\n",
       "      <th>model_name</th>\n",
       "      <th>temperature</th>\n",
       "      <th>sim_score_1</th>\n",
       "      <th>sim_score_2</th>\n",
       "      <th>sim_diff</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Mexico</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Poland</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China-Albania</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[14, 14]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan-Philippines</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[16, 16]</td>\n",
       "      <td>[16, 16]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Canada</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[14, 14]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>England-Ireland</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[19, 19]</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany-Austria</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-France</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belgium-Luxembourg</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[19, 19]</td>\n",
       "      <td>[19, 19]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-U.S.S.R.</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China-North Korea</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[17, 17]</td>\n",
       "      <td>[17, 17]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India-Sri Lanka</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-France</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[18, 18]</td>\n",
       "      <td>[-3, -3]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Cuba</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[14, 14]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>England-Jordan</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[17, 17]</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Germany</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[15, 15]</td>\n",
       "      <td>[16, 16]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Syria</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country_pair prompt_style  model_name  temperature sim_score_1  \\\n",
       "0       U.S.A.-Mexico       single  mistral_7B        0.001    [18, 18]   \n",
       "0     U.S.S.R.-Poland       single  mistral_7B        0.001    [15, 15]   \n",
       "0       China-Albania       single  mistral_7B        0.001    [15, 15]   \n",
       "0       U.S.A.-Israel       single  mistral_7B        0.001    [18, 18]   \n",
       "0   Japan-Philippines       single  mistral_7B        0.001    [16, 16]   \n",
       "0       U.S.A.-Canada       single  mistral_7B        0.001    [18, 18]   \n",
       "0     U.S.S.R.-Israel       single  mistral_7B        0.001    [15, 15]   \n",
       "0     England-Ireland       single  mistral_7B        0.001    [19, 19]   \n",
       "0     Germany-Austria       single  mistral_7B        0.001    [18, 18]   \n",
       "0     U.S.S.R.-France       single  mistral_7B        0.001    [10, 10]   \n",
       "0  Belgium-Luxembourg       single  mistral_7B        0.001    [19, 19]   \n",
       "0     U.S.A.-U.S.S.R.       single  mistral_7B        0.001    [10, 10]   \n",
       "0   China-North Korea       single  mistral_7B        0.001    [17, 17]   \n",
       "0     India-Sri Lanka       single  mistral_7B        0.001    [18, 18]   \n",
       "0       U.S.A.-France       single  mistral_7B        0.001    [15, 15]   \n",
       "0       U.S.S.R.-Cuba       single  mistral_7B        0.001    [15, 15]   \n",
       "0      England-Jordan       single  mistral_7B        0.001    [15, 15]   \n",
       "0       France-Israel       single  mistral_7B        0.001    [17, 17]   \n",
       "0      U.S.A.-Germany       single  mistral_7B        0.001    [15, 15]   \n",
       "0      U.S.S.R.-Syria       single  mistral_7B        0.001      [1, 1]   \n",
       "\n",
       "  sim_score_2  sim_diff p-values  \n",
       "0    [18, 18]    [0, 0]       []  \n",
       "0    [10, 10]    [5, 5]       []  \n",
       "0      [1, 1]  [14, 14]       []  \n",
       "0    [18, 18]    [0, 0]       []  \n",
       "0    [16, 16]    [0, 0]       []  \n",
       "0    [18, 18]    [0, 0]       []  \n",
       "0      [1, 1]  [14, 14]       []  \n",
       "0    [18, 18]    [1, 1]       []  \n",
       "0    [18, 18]    [0, 0]       []  \n",
       "0    [10, 10]    [0, 0]       []  \n",
       "0    [19, 19]    [0, 0]       []  \n",
       "0    [10, 10]    [0, 0]       []  \n",
       "0    [17, 17]    [0, 0]       []  \n",
       "0    [18, 18]    [0, 0]       []  \n",
       "0    [18, 18]  [-3, -3]       []  \n",
       "0      [1, 1]  [14, 14]       []  \n",
       "0    [15, 15]    [0, 0]       []  \n",
       "0    [15, 15]    [2, 2]       []  \n",
       "0    [16, 16]  [-1, -1]       []  \n",
       "0      [1, 1]    [0, 0]       []  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_score_1_all = df.iloc[0, 4]\n",
    "sim_score_2_all = df.iloc[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sim_score_1_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_path, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model deleted..\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "print('model deleted..')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
