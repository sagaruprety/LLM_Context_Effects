{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama, HuggingFaceHub, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# print(GPUtil.showUtilization())\n",
    "# OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-Y2nqeGoOCxTZnITAzuFdT3BlbkFJf6Rm2tmkcssXIov3PMFQ'\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"ls__93636df794f14ccba7162354d46779d8\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"LLM_Context_Effects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf_repo_id_mapping = {\n",
    "    'mistral_7B': \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    'llama2_7B':'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'llama2_13B': 'meta-llama/Llama-2-13b-chat-hf',\n",
    "    'llama2_70B': 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    'llama3_8B':'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'llama3_70B': 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "}\n",
    "\n",
    "model_ollama_id_mapping = {\n",
    "    'mistral_7B': \"mistral:7b-instruct-fp16\",\n",
    "    'llama2_7B': 'llama2:7b-chat-fp16',\n",
    "    'llama2_13B': 'llama2:13b-chat-fp16',\n",
    "    'llama2_70B': 'llama2:70b-chat-fp16',\n",
    "    'llama3_8B':'llama3:8b-instruct-fp16',\n",
    "    'llama3_70B': 'llama3:70b-instruct-fp16'\n",
    "}\n",
    "\n",
    "model_name_type_mapping={\n",
    "    'gpt-3.5-turbo': 'openai',\n",
    "    'gpt-4': 'openai',\n",
    "    'mistral_7B': 'open-source',\n",
    "    'llama2_7B': 'open-source',\n",
    "    'llama2_13B': 'open-source',\n",
    "    'llama2_70B': 'open-source',\n",
    "    'llama3_8B': 'open-source',\n",
    "    'llama3_70B': 'open-source',\n",
    "}\n",
    "\n",
    "def initialise_openai_models(model_name, temperature):\n",
    "    model = ChatOpenAI(model=model_name, api_key=os.environ[\"OPENAI_API_KEY\"], temperature=temperature, max_tokens=20)\n",
    "    return model\n",
    "\n",
    "def initialise_open_source_models_transformers(model_name, temperature):\n",
    "    # Use a pipeline as a high-level helper\n",
    "    repo_id = model_hf_repo_id_mapping[model_name]\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                    model=repo_id,\n",
    "                    token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "                    device_map = \"sequential\", max_new_tokens = 22,\n",
    "                    do_sample = True,\n",
    "                    return_full_text = False,\n",
    "                    temperature = temperature,\n",
    "                    top_k = 50,\n",
    "                    top_p = 0.9)\n",
    "    return  HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def initialise_open_source_models_ollama(model_name, temperature):\n",
    "    ollama_id = model_ollama_id_mapping[model_name]\n",
    "    model = Ollama(base_url='http://localhost:11434',\n",
    "    model=ollama_id, temperature = temperature, num_predict = 20, format = 'json', num_gpu=-1)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def initialise_models(model_name = 'mistral_7B', model_type = 'openai', temperature= 0.0):\n",
    "    if model_type == 'openai':\n",
    "        return initialise_openai_models(model_name, temperature)\n",
    "    else:\n",
    "        return initialise_open_source_models_ollama(model_name, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U.S.A.-Mexico': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Mexico? Shape: score: int', 'U.S.S.R.-Poland': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Poland? Shape: score: int', 'China-Albania': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is China similar to Albania? Shape: score: int', 'U.S.A.-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Israel? Shape: score: int', 'Japan-Philippines': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is Japan similar to Philippines? Shape: score: int', 'U.S.A.-Canada': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Canada? Shape: score: int', 'U.S.S.R.-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Israel? Shape: score: int', 'England-Ireland': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is England similar to Ireland? Shape: score: int', 'Germany-Austria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is Germany similar to Austria? Shape: score: int', 'U.S.S.R.-France': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to France? Shape: score: int', 'Belgium-Luxembourg': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is Belgium similar to Luxembourg? Shape: score: int', 'U.S.A.-U.S.S.R.': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to U.S.S.R.? Shape: score: int', 'China-North Korea': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is China similar to North Korea? Shape: score: int', 'India-Sri Lanka': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is India similar to Sri Lanka? Shape: score: int', 'U.S.A.-France': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to France? Shape: score: int', 'U.S.S.R.-Cuba': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Cuba? Shape: score: int', 'England-Jordan': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is England similar to Jordan? Shape: score: int', 'France-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is France similar to Israel? Shape: score: int', 'U.S.A.-Germany': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.A. similar to Germany? Shape: score: int', 'U.S.S.R.-Syria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is U.S.S.R. similar to Syria? Shape: score: int', 'France-Algeria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is France similar to Algeria? Shape: score: int'}\n"
     ]
    }
   ],
   "source": [
    "temperatures = [0.001, 0.5, 1.0, 1.5]\n",
    "similarity_effect_country_pairs = [\n",
    "('U.S.A.', 'Mexico'),\n",
    "('U.S.S.R.', 'Poland'),\n",
    "('China', 'Albania'),\n",
    "('U.S.A.', 'Israel'),\n",
    "('Japan', 'Philippines'),\n",
    "('U.S.A.', 'Canada'),\n",
    "('U.S.S.R.', 'Israel'),\n",
    "('England', 'Ireland'),\n",
    "('Germany', 'Austria'),\n",
    "('U.S.S.R.', 'France'),\n",
    "('Belgium', 'Luxembourg'),\n",
    "('U.S.A.', 'U.S.S.R.'),\n",
    "('China', 'North Korea'),\n",
    "('India', 'Sri Lanka'),\n",
    "('U.S.A.', 'France'),\n",
    "('U.S.S.R.', 'Cuba'),\n",
    "('England', 'Jordan'),\n",
    "('France', 'Israel'),\n",
    "('U.S.A.', 'Germany'),\n",
    "('U.S.S.R.', 'Syria'),\n",
    "('France', 'Algeria')]\n",
    "\n",
    "questions_order_1 = {}\n",
    "questions_order_2 = {}\n",
    "\n",
    "questions_order_similar_to_1 = {}\n",
    "questions_order_similar_to_2 = {}\n",
    "questions_order_similar_degree_1 = {}\n",
    "questions_order_similar_degree_2 = {}\n",
    "for country1, country2 in similarity_effect_country_pairs:\n",
    "    order_1 = f'{country1}-{country2}'\n",
    "    order_2 = f'{country2}-{country1}'\n",
    "    questions_order_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country1} and {country2}? Shape: score: int\"\n",
    "    questions_order_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country2} and {country1}? Shape: score: int\"\n",
    "    questions_order_similar_to_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country1} similar to {country2}? Shape: score: int\"\n",
    "    questions_order_similar_to_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country2} similar to {country1}? Shape: score: int\"\n",
    "    questions_order_similar_degree_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country1} similar to {country2}? Shape: score: int\"\n",
    "    questions_order_similar_degree_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country2} similar to {country1}? Shape: score: int\"\n",
    "print(questions_order_similar_to_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following two questions to the best of your knowledge. In each case, your answer should be a well-formed json of shape provided.\n",
    "Question 1: {text_1}\n",
    "Question 2: {text_2}\n",
    "Answer: score_1: int, score_2: int\n",
    "\"\"\"\n",
    "modified_template = template = \"\"\"Answer the following two questions to the best of your knowledge. In each case, your answer should be a well-formed json of shape provided.\n",
    "                                Question 1: {text_1}\n",
    "                                Question 2: {text_2}\n",
    "                                Answer: score_1: int, score_2: int\n",
    "                                Please provide integer score to both the questions. \n",
    "                                \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "modified_prompt = ChatPromptTemplate.from_template(modified_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'mistral_7B',\n",
    "     'llama2_7B',\n",
    "  'llama3_8B',\n",
    " 'llama2_13B',\n",
    " 'gpt-3.5-turbo',\n",
    " 'gpt-4']\n",
    "\n",
    "models = ['llama2_70B']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_numeric_output(raw_output):\n",
    "    match = re.search(r'\\d+', raw_output)\n",
    "    # print(match, match.group())\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n",
    "def parse_json_to_numeric(raw_output):\n",
    "# Define the regular expression pattern\n",
    "    pattern = r'\"score_\\d+\": (\\d+)'\n",
    "\n",
    "    # Find all matches using the pattern\n",
    "    matches = re.findall(pattern, raw_output)\n",
    "\n",
    "    # Convert the matches to integers\n",
    "    scores = [int(match) for match in matches]\n",
    "\n",
    "    # Print the extracted integer values\n",
    "    print(\"Integer values extracted from JSON:\", scores)\n",
    "    return scores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_columns = {\n",
    "    'country_pair': '',\n",
    "    'prompt_style': 'dual',\n",
    "    'model_name': '',\n",
    "    'temperature': '',\n",
    "    'sim_score_1': [],\n",
    "    'sim_score_2': [],\n",
    "    'sim_diff': [],\n",
    "    'p-values': []\n",
    "}\n",
    "# Define the file path\n",
    "file_path = './results/results_dual_prompt_similar_degree.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    # Create an empty DataFrame from the dictionary\n",
    "    df = pd.DataFrame(columns=results_dict_columns)\n",
    "\n",
    "# Print the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(prompt, model, order, model_name, temperature):\n",
    "    return (prompt | model | StrOutputParser()).with_config({\n",
    "\"metadata\": {\n",
    "    'country-pair-order': order,\n",
    "    'model_name':model_name,\n",
    "    'temperature': temperature,\n",
    "    'prompt_style': 'dual'\n",
    "}}\n",
    ")\n",
    "\n",
    "def get_output(prompt, model, order, model_name, temperature, ques_1, ques_2):\n",
    "    chain = create_chain(prompt, model, order, model_name, temperature)\n",
    "    # print(ques_1)\n",
    "    return chain.invoke({\"text_1\": ques_1, \"text_2\": ques_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 20, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.001, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 0.001, output1: 10, output2: 15\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 0.001, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 0.001, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 0.001, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 0.001, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 0.001, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 0.001, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 0.001, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 0.001, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 0.001, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 0.001, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 0.001, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 0.001, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 0.001, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 0.001, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 0.001, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 0.001, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 0.001, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 0.001, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 5,\n",
      "\"score_2\": 8\n",
      "}\n",
      "Integer values extracted from JSON: [5, 8]\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 0.001, output1: 5, output2: 8\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 0.001, output1: 10, output2: 15\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 20, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 0.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 0.5, output1: 10, output2: 15\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 0.5, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 5,\n",
      "\"score_2\": 8\n",
      "}\n",
      "Integer values extracted from JSON: [5, 8]\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 0.5, output1: 5, output2: 8\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 0.5, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 0.5, output1: 10, output2: 15\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 0.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 0.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 0.5, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 0.5, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 0.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 0.5, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [12, 18]\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 0.5, output1: 12, output2: 18\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 0.5, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 0.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 0.5, output1: 10, output2: 15\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 0.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 0.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 0.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 0.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 5,\n",
      "\"score_2\": 8\n",
      "}\n",
      "Integer values extracted from JSON: [5, 8]\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 0.5, output1: 5, output2: 8\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 0.5, output1: 12, output2: 15\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 20, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 1.0, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 1.0, output1: 10, output2: 15\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 8\n",
      "\n",
      "Integer values extracted from JSON: [10, 8]\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 1.0, output1: 10, output2: 8\n",
      "{\n",
      "\"score_1\": 5,\n",
      "\"score_2\": 8\n",
      "}\n",
      "Integer values extracted from JSON: [5, 8]\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 1.0, output1: 5, output2: 8\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 1.0, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 1.0, output1: 10, output2: 15\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 1.0, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 1.0, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 1.0, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 1.0, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 1.0, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 1.0, output1: 15, output2: 17\n",
      "{}\n",
      "Integer values extracted from JSON: []\n",
      "Issue with output parsing for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 1.0 output: {}. Modifying prompt and calling LLM again \n",
      "new output with modified prompt is {\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [12, 18]\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 1.0, output1: 12, output2: 18\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [12, 17]\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 1.0, output1: 12, output2: 17\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 1.0, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [12, 17]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 1.0, output1: 12, output2: 17\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 1.0, output1: 15, output2: 17\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 1.0, output1: 10, output2: 15\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 1.0, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 1.0, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 5,\n",
      "\"score_2\": 8\n",
      "}\n",
      "Integer values extracted from JSON: [5, 8]\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 1.0, output1: 5, output2: 8\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 1.0, output1: 10, output2: 15\n",
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'llama2:70b-chat-fp16', 'format': 'json', 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': -1, 'num_thread': None, 'num_predict': 20, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': 1.5, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None}\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [12, 18]\n",
      "for Model_name: llama2_70B, Pair: Mexico-U.S.A., Temperature: 1.5, output1: 12, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 6\n",
      "}\n",
      "Integer values extracted from JSON: [8, 6]\n",
      "for Model_name: llama2_70B, Pair: Poland-U.S.S.R., Temperature: 1.5, output1: 8, output2: 6\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Albania-China, Temperature: 1.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.A., Temperature: 1.5, output1: 12, output2: 15\n",
      "{\n",
      "  \"score_1\": 10,\n",
      "  \"score_2\": \n",
      "Integer values extracted from JSON: [10]\n",
      "Issue with output parsing for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 1.5 output: {\n",
      "  \"score_1\": 10,\n",
      "  \"score_2\": . Modifying prompt and calling LLM again \n",
      "new output with modified prompt is {\n",
      "  \"score_1\": 12,\n",
      "  \"score_2\": \n",
      "Integer values extracted from JSON: [12]\n",
      "Issue with output parsing for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 1.5 output: {\n",
      "  \"score_1\": 12,\n",
      "  \"score_2\": . Modifying prompt and calling LLM again \n",
      "new output with modified prompt is {\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Philippines-Japan, Temperature: 1.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Canada-U.S.A., Temperature: 1.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-U.S.S.R., Temperature: 1.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [12, 17]\n",
      "for Model_name: llama2_70B, Pair: Ireland-England, Temperature: 1.5, output1: 12, output2: 17\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Austria-Germany, Temperature: 1.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.S.R., Temperature: 1.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 17\n",
      "Integer values extracted from JSON: [15, 17]\n",
      "for Model_name: llama2_70B, Pair: Luxembourg-Belgium, Temperature: 1.5, output1: 15, output2: 17\n",
      "{}\n",
      "Integer values extracted from JSON: []\n",
      "Issue with output parsing for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 1.5 output: {}. Modifying prompt and calling LLM again \n",
      "new output with modified prompt is {\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: U.S.S.R.-U.S.A., Temperature: 1.5, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 8\n",
      "\n",
      "Integer values extracted from JSON: [10, 8]\n",
      "for Model_name: llama2_70B, Pair: North Korea-China, Temperature: 1.5, output1: 10, output2: 8\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Sri Lanka-India, Temperature: 1.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 10\n",
      "Integer values extracted from JSON: [15, 10]\n",
      "for Model_name: llama2_70B, Pair: France-U.S.A., Temperature: 1.5, output1: 15, output2: 10\n",
      "{\n",
      "\"score_1\": 15,\n",
      "\"score_2\": 18\n",
      "Integer values extracted from JSON: [15, 18]\n",
      "for Model_name: llama2_70B, Pair: Cuba-U.S.S.R., Temperature: 1.5, output1: 15, output2: 18\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Jordan-England, Temperature: 1.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Israel-France, Temperature: 1.5, output1: 8, output2: 12\n",
      "{\n",
      "\"score_1\": 12,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [12, 15]\n",
      "for Model_name: llama2_70B, Pair: Germany-U.S.A., Temperature: 1.5, output1: 12, output2: 15\n",
      "{\n",
      "\"score_1\": 8,\n",
      "\"score_2\": 12\n",
      "\n",
      "Integer values extracted from JSON: [8, 12]\n",
      "for Model_name: llama2_70B, Pair: Syria-U.S.S.R., Temperature: 1.5, output1: 8, output2: 12\n",
      "{\n",
      "  \"score_1\": 10,\n",
      "  \"score_2\": \n",
      "Integer values extracted from JSON: [10]\n",
      "Issue with output parsing for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 1.5 output: {\n",
      "  \"score_1\": 10,\n",
      "  \"score_2\": . Modifying prompt and calling LLM again \n",
      "new output with modified prompt is {\n",
      "  \"score_1\": 10,\n",
      "  \"score_2\": \n",
      "Integer values extracted from JSON: [10]\n",
      "Issue with output parsing for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 1.5 output: {\n",
      "  \"score_1\": 10,\n",
      "  \"score_2\": . Modifying prompt and calling LLM again \n",
      "new output with modified prompt is {\n",
      "\"score_1\": 10,\n",
      "\"score_2\": 15\n",
      "Integer values extracted from JSON: [10, 15]\n",
      "for Model_name: llama2_70B, Pair: Algeria-France, Temperature: 1.5, output1: 10, output2: 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name in models:\n",
    "    model_type = model_name_type_mapping[model_name]\n",
    "    results_dict_columns['model_name'] = model_name\n",
    "    for temperature in temperatures:\n",
    "        model = initialise_models(model_name, model_type, temperature)\n",
    "        results_dict_columns['temperature'] = temperature\n",
    "        for order_1, order_2 in zip(questions_order_similar_to_1, questions_order_similar_to_2):\n",
    "            # print(order_1, order_2)\n",
    "            results_dict_columns['country_pair'] = order_1\n",
    "            ques_1 = questions_order_similar_degree_1[order_1]\n",
    "            ques_2 = questions_order_similar_degree_2[order_2]\n",
    "            output = get_output(prompt, model, order_1, model_name, temperature, ques_1, ques_2)\n",
    "            print(output)\n",
    "            parsed_output = parse_json_to_numeric(output)\n",
    "            if parsed_output:\n",
    "                if len(parsed_output) == 2:\n",
    "                    sim_score_1, sim_score_2 = parsed_output\n",
    "            retry_count = 0\n",
    "            while(not parsed_output or len(parsed_output) != 2 or\n",
    "                    parsed_output[0] > 20 or parsed_output[0] < 0\n",
    "                      or parsed_output[1] > 20 or parsed_output[1] < 0):\n",
    "                print(f'Issue with output parsing for Model_name: {model_name}, Pair: {order_2}, Temperature: {temperature} output: {output}. Modifying prompt and calling LLM again ')\n",
    "                output = get_output(modified_prompt, model, order_1, model_name, temperature, ques_1, ques_2)\n",
    "                print(f'new output with modified prompt is {output}')\n",
    "                parsed_output = parse_json_to_numeric(output)\n",
    "                retry_count+=1\n",
    "                if retry_count == 5:\n",
    "                    print('Tried to modify prompt too many times, now giving up')\n",
    "                    break\n",
    "            if  retry_count!=5:\n",
    "                sim_score_1, sim_score_2 = parsed_output\n",
    "            else:\n",
    "                sim_score_1 = sim_score_2 = None\n",
    "                print(f' cannot parse output {output} for Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}')\n",
    "\n",
    "            if sim_score_1 != None and sim_score_2 != None:\n",
    "                sim_diff = sim_score_1 - sim_score_2\n",
    "            else:\n",
    "                sim_diff = None\n",
    "            print(f'for Model_name: {model_name}, Pair: {order_2}, Temperature: {temperature}, output1: {sim_score_1}, output2: {sim_score_2}')\n",
    "            results_dict_columns['sim_score_1'] = sim_score_1\n",
    "            results_dict_columns['sim_score_2'] = sim_score_2\n",
    "            results_dict_columns['sim_diff'] = sim_diff\n",
    "            df = pd.concat([df, pd.DataFrame.from_dict([results_dict_columns])])\n",
    "        # df.to_csv(file_path, index=False, mode='a')    \n",
    "    # del model\n",
    "        # print('model deleted..')\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_path, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model deleted..\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "print('model deleted..')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
