{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "# import GPUtil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama, HuggingFaceHub, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# print(GPUtil.showUtilization())\n",
    "# OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-Y2nqeGoOCxTZnITAzuFdT3BlbkFJf6Rm2tmkcssXIov3PMFQ'\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"ls__93636df794f14ccba7162354d46779d8\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"LLM_Context_Effects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf_repo_id_mapping = {\n",
    "    'mistral_7B': \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    'llama2_7B':'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'llama2_13B': 'meta-llama/Llama-2-13b-chat-hf',\n",
    "    'llama2_70B': 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    'llama3_8B':'meta-llama/Meta-Llama-3-8B',\n",
    "    'llama3_70B': 'meta-llama/Meta-Llama-3-70B'\n",
    "}\n",
    "\n",
    "model_name_type_mapping={\n",
    "    'gpt-3.5-turbo': 'openai',\n",
    "    'gpt-4': 'openai',\n",
    "    'mistral_7B': 'open-source',\n",
    "    'llama2_7B': 'open-source',\n",
    "    'llama2_13B': 'open-source',\n",
    "    'llama2_70B': 'open-source',\n",
    "    'llama3_8B': 'open-source',\n",
    "    'llama3_70B': 'open-source',\n",
    "}\n",
    "\n",
    "def initialise_openai_models(model_name, temperature):\n",
    "    model = ChatOpenAI(model=model_name, api_key=os.environ[\"OPENAI_API_KEY\"], temperature=temperature)\n",
    "    return model\n",
    "\n",
    "def initialise_open_source_models_transformers(model_name, temperature):\n",
    "    # Use a pipeline as a high-level helper\n",
    "    repo_id = model_hf_repo_id_mapping[model_name]\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                    model=repo_id,\n",
    "                    token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "                    device_map = \"balanced\", max_new_tokens = 10,\n",
    "                    return_full_text= False)\n",
    "    return  HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def initialise_open_source_models(model_name, temperature):\n",
    "    repo_id = model_hf_repo_id_mapping[model_name]\n",
    "    model = HuggingFaceHub(    \n",
    "        repo_id=repo_id,\n",
    "        task=\"text-generation\",\n",
    "        model_kwargs={\n",
    "            \"max_new_tokens\": 10,\n",
    "            \"temperature\": temperature,\n",
    "            \"device_map\": 'balanced',\n",
    "            'include_prompt_in_result': False\n",
    "        },\n",
    "        )\n",
    "    # model = Ollama(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def initialise_models(model_name = 'mistral_7B', model_type = 'openai', temperature= 0.0):\n",
    "    if model_type == 'openai':\n",
    "        return initialise_openai_models(model_name, temperature)\n",
    "    else:\n",
    "        return initialise_open_source_models_transformers(model_name, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U.S.A.-Mexico': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.A. and Mexico? Return only a number', 'U.S.S.R.-Poland': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.S.R. and Poland? Return only a number', 'China-Albania': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are China and Albania? Return only a number', 'U.S.A.-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.A. and Israel? Return only a number', 'Japan-Philippines': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are Japan and Philippines? Return only a number', 'U.S.A.-Canada': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.A. and Canada? Return only a number', 'U.S.S.R.-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.S.R. and Israel? Return only a number', 'England-Ireland': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are England and Ireland? Return only a number', 'Germany-Austria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are Germany and Austria? Return only a number', 'U.S.S.R.-France': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.S.R. and France? Return only a number', 'Belgium-Luxembourg': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are Belgium and Luxembourg? Return only a number', 'U.S.A.-U.S.S.R.': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.A. and U.S.S.R.? Return only a number', 'China-North Korea': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are China and North Korea? Return only a number', 'India-Sri Lanka': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are India and Sri Lanka? Return only a number', 'U.S.A.-France': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.A. and France? Return only a number', 'U.S.S.R.-Cuba': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.S.R. and Cuba? Return only a number', 'England-Jordan': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are England and Jordan? Return only a number', 'France-Israel': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are France and Israel? Return only a number', 'U.S.A.-Germany': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.A. and Germany? Return only a number', 'U.S.S.R.-Syria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.S.R. and Syria? Return only a number', 'France-Algeria': 'On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are France and Algeria? Return only a number'}\n"
     ]
    }
   ],
   "source": [
    "temperatures = [0.001, 0.5, 1.0, 1.5]\n",
    "similarity_effect_country_pairs = [\n",
    "('U.S.A.', 'Mexico'),\n",
    "('U.S.S.R.', 'Poland'),\n",
    "('China', 'Albania'),\n",
    "('U.S.A.', 'Israel'),\n",
    "('Japan', 'Philippines'),\n",
    "('U.S.A.', 'Canada'),\n",
    "('U.S.S.R.', 'Israel'),\n",
    "('England', 'Ireland'),\n",
    "('Germany', 'Austria'),\n",
    "('U.S.S.R.', 'France'),\n",
    "('Belgium', 'Luxembourg'),\n",
    "('U.S.A.', 'U.S.S.R.'),\n",
    "('China', 'North Korea'),\n",
    "('India', 'Sri Lanka'),\n",
    "('U.S.A.', 'France'),\n",
    "('U.S.S.R.', 'Cuba'),\n",
    "('England', 'Jordan'),\n",
    "('France', 'Israel'),\n",
    "('U.S.A.', 'Germany'),\n",
    "('U.S.S.R.', 'Syria'),\n",
    "('France', 'Algeria')]\n",
    "\n",
    "questions_order_1 = {}\n",
    "questions_order_2 = {}\n",
    "\n",
    "for country1, country2 in similarity_effect_country_pairs:\n",
    "    order_1 = f'{country1}-{country2}'\n",
    "    order_2 = f'{country2}-{country1}'\n",
    "    questions_order_1[order_1] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country1} and {country2}? Return only a number\"\n",
    "    questions_order_2[order_2] = f\"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country2} and {country1}? Return only a number\"\n",
    "\n",
    "print(questions_order_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are U.S.A. and Mexico? Return only a number\n",
      "Mexico-U.S.A.\n"
     ]
    }
   ],
   "source": [
    "for order_1, order_2 in zip(questions_order_1, questions_order_2):\n",
    "    print(questions_order_1[order_1])\n",
    "    print(order_2)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question in as few words as possible. \n",
    "If the question specifies the options to choose from, only output that option and no other word or token.\n",
    "Your response should only be a number between 0-20 and nothing else.\n",
    "Text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['mistral_7B',\n",
    " 'llama2_7B',\n",
    " 'llama2_13B',\n",
    " 'llama2_70B',\n",
    " 'llama3_8B',\n",
    " 'llama3_70B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_numeric_output(raw_output):\n",
    "    match = re.search(r'\\d+', raw_output)\n",
    "    # print(match, match.group())\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          country_pair prompt_style  model_name  temperature  sim_score_1  \\\n",
      "0        U.S.A.-Mexico       single  mistral_7B        0.001           12   \n",
      "1      U.S.S.R.-Poland       single  mistral_7B        0.001           10   \n",
      "2        China-Albania       single  mistral_7B        0.001            0   \n",
      "3        U.S.A.-Israel       single  mistral_7B        0.001           15   \n",
      "4    Japan-Philippines       single  mistral_7B        0.001           10   \n",
      "5        U.S.A.-Canada       single  mistral_7B        0.001           18   \n",
      "6      U.S.S.R.-Israel       single  mistral_7B        0.001            0   \n",
      "7      England-Ireland       single  mistral_7B        0.001            8   \n",
      "8      Germany-Austria       single  mistral_7B        0.001           15   \n",
      "9      U.S.S.R.-France       single  mistral_7B        0.001            6   \n",
      "10  Belgium-Luxembourg       single  mistral_7B        0.001           18   \n",
      "11     U.S.A.-U.S.S.R.       single  mistral_7B        0.001            0   \n",
      "12   China-North Korea       single  mistral_7B        0.001           10   \n",
      "13     India-Sri Lanka       single  mistral_7B        0.001           15   \n",
      "14       U.S.A.-France       single  mistral_7B        0.001           15   \n",
      "15       U.S.S.R.-Cuba       single  mistral_7B        0.001           15   \n",
      "16      England-Jordan       single  mistral_7B        0.001            4   \n",
      "17       France-Israel       single  mistral_7B        0.001           10   \n",
      "18      U.S.A.-Germany       single  mistral_7B        0.001           12   \n",
      "19      U.S.S.R.-Syria       single  mistral_7B        0.001            5   \n",
      "20      France-Algeria       single  mistral_7B        0.001           10   \n",
      "\n",
      "    sim_score_2  sim_diff  p-value  \n",
      "0            15        -3      NaN  \n",
      "1             0        10      NaN  \n",
      "2             0         0      NaN  \n",
      "3            18        -3      NaN  \n",
      "4             6         4      NaN  \n",
      "5            15         3      NaN  \n",
      "6             0         0      NaN  \n",
      "7             8         0      NaN  \n",
      "8            18        -3      NaN  \n",
      "9             0         6      NaN  \n",
      "10           18         0      NaN  \n",
      "11            0         0      NaN  \n",
      "12           15        -5      NaN  \n",
      "13           18        -3      NaN  \n",
      "14           10         5      NaN  \n",
      "15            0        15      NaN  \n",
      "16            0         4      NaN  \n",
      "17           12        -2      NaN  \n",
      "18           13        -1      NaN  \n",
      "19            0         5      NaN  \n",
      "20           15        -5      NaN  \n"
     ]
    }
   ],
   "source": [
    "results_dict_columns = {\n",
    "    'country_pair': '',\n",
    "    'prompt_style': 'single',\n",
    "    'model_name': '',\n",
    "    'temperature': '',\n",
    "    'sim_score_1': '',\n",
    "    'sim_score_2': '',\n",
    "    'sim_diff': '',\n",
    "    'p-value': ''\n",
    "}\n",
    "# Define the file path\n",
    "file_path = './results/results.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    # Create an empty DataFrame from the dictionary\n",
    "    df = pd.DataFrame(columns=results_dict_columns)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccaesup/miniconda3/envs/LLM_Context_Effects/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e094a8db0d44171976c48785cfd1245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.A.-Mexico', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 12, 'sim_score_2': 15, 'sim_diff': -3, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.S.R.-Poland', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 10, 'sim_score_2': 0, 'sim_diff': 10, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'China-Albania', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 0, 'sim_score_2': 0, 'sim_diff': 0, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.A.-Israel', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 15, 'sim_score_2': 18, 'sim_diff': -3, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'Japan-Philippines', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 10, 'sim_score_2': 6, 'sim_diff': 4, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.A.-Canada', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 18, 'sim_score_2': 15, 'sim_diff': 3, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.S.R.-Israel', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 0, 'sim_score_2': 0, 'sim_diff': 0, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'England-Ireland', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 8, 'sim_score_2': 8, 'sim_diff': 0, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'Germany-Austria', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 15, 'sim_score_2': 18, 'sim_diff': -3, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.S.R.-France', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 6, 'sim_score_2': 0, 'sim_diff': 6, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'Belgium-Luxembourg', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 18, 'sim_score_2': 18, 'sim_diff': 0, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.A.-U.S.S.R.', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 0, 'sim_score_2': 0, 'sim_diff': 0, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'China-North Korea', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 10, 'sim_score_2': 15, 'sim_diff': -5, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'India-Sri Lanka', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 15, 'sim_score_2': 18, 'sim_diff': -3, 'p-value': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country_pair': 'U.S.A.-France', 'prompt_style': 'single', 'model_name': 'mistral_7B', 'temperature': 0.5, 'sim_score_1': 15, 'sim_score_2': 10, 'sim_diff': 5, 'p-value': ''}\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = None\n",
    "for model_name in models[0:1]:\n",
    "    model_type = model_name_type_mapping[model_name]\n",
    "    results_dict_columns['model_name'] = model_name\n",
    "    for temperature in temperatures[1:]:\n",
    "            if model:\n",
    "                del model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            model = initialise_models(model_name, model_type, temperature)\n",
    "            results_dict_columns['temperature'] = temperature\n",
    "            for order_1, order_2 in zip(questions_order_1, questions_order_2):\n",
    "                results_dict_columns['country_pair'] = order_1\n",
    "                \n",
    "\n",
    "                ques_1 = questions_order_1[order_1]\n",
    "                ques_2 = questions_order_2[order_2]\n",
    "                chain_1 = prompt | model | StrOutputParser().with_config({\n",
    "            \"metadata\": {\n",
    "                    'country-pair-order': order_1,\n",
    "                    'model_name':model_name,\n",
    "                    'temperature': temperature,\n",
    "                }}\n",
    "                )\n",
    "                chain_2 = prompt | model | StrOutputParser().with_config({\n",
    "            \"metadata\": {\n",
    "                    'country-pair-order': order_2,\n",
    "                    'model_name':model_name,\n",
    "                    'temperature': temperature,\n",
    "                }}\n",
    "                )\n",
    "                output_1 = chain_1.invoke({\"text\": ques_1})\n",
    "                output_2 = chain_2.invoke({\"text\": ques_2})\n",
    "                # output = chain.invoke({\"text\": order_2})\n",
    "                parsed_output_1 = parse_numeric_output(output_1)\n",
    "                if  parsed_output_1:\n",
    "                     sim_score_1 =int(parsed_output_1)\n",
    "                else:\n",
    "                    sim_score_1 = None\n",
    "                    print(f' cannot parse output {output_1} for Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}')\n",
    "                parsed_output_2 = parse_numeric_output(output_2)\n",
    "                if parsed_output_2:\n",
    "                    sim_score_2 =int(parsed_output_2)\n",
    "                else:\n",
    "                    print(f' cannot parse output {output_2} for Model_name: {model_name}, Pair: {order_2}, Temperature: {temperature}')\n",
    "                    sim_score_2 = None\n",
    "                results_dict_columns['sim_score_1'] = sim_score_1\n",
    "                results_dict_columns['sim_score_2'] = sim_score_2\n",
    "                if sim_score_1!=None and sim_score_2!=None:\n",
    "                    results_dict_columns['sim_diff'] = sim_score_1 - sim_score_2\n",
    "                else:\n",
    "                    results_dict_columns['sim_diff'] = None\n",
    "                # print(f'Model_name: {model_name}, Pair: {order_1}, Temperature: {temperature}, output1: {parsed_output_1}, output2: {parsed_output_2}')\n",
    "                print(results_dict_columns)\n",
    "                df = pd.concat([df, pd.DataFrame.from_dict([results_dict_columns])])\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_pair</th>\n",
       "      <th>prompt_style</th>\n",
       "      <th>model_name</th>\n",
       "      <th>temperature</th>\n",
       "      <th>sim_score_1</th>\n",
       "      <th>sim_score_2</th>\n",
       "      <th>sim_diff</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Mexico</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>-3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Poland</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China-Albania</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>-3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan-Philippines</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Canada</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>England-Ireland</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany-Austria</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>-3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-France</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belgium-Luxembourg</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-U.S.S.R.</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China-North Korea</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>-5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India-Sri Lanka</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>-3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-France</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Cuba</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>England-Jordan</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France-Israel</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>-2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.A.-Germany</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.S.R.-Syria</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France-Algeria</td>\n",
       "      <td>single</td>\n",
       "      <td>mistral_7B</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>-5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country_pair prompt_style  model_name  temperature sim_score_1  \\\n",
       "0       U.S.A.-Mexico       single  mistral_7B        0.001          12   \n",
       "0     U.S.S.R.-Poland       single  mistral_7B        0.001          10   \n",
       "0       China-Albania       single  mistral_7B        0.001           0   \n",
       "0       U.S.A.-Israel       single  mistral_7B        0.001          15   \n",
       "0   Japan-Philippines       single  mistral_7B        0.001          10   \n",
       "0       U.S.A.-Canada       single  mistral_7B        0.001          18   \n",
       "0     U.S.S.R.-Israel       single  mistral_7B        0.001           0   \n",
       "0     England-Ireland       single  mistral_7B        0.001           8   \n",
       "0     Germany-Austria       single  mistral_7B        0.001          15   \n",
       "0     U.S.S.R.-France       single  mistral_7B        0.001           6   \n",
       "0  Belgium-Luxembourg       single  mistral_7B        0.001          18   \n",
       "0     U.S.A.-U.S.S.R.       single  mistral_7B        0.001           0   \n",
       "0   China-North Korea       single  mistral_7B        0.001          10   \n",
       "0     India-Sri Lanka       single  mistral_7B        0.001          15   \n",
       "0       U.S.A.-France       single  mistral_7B        0.001          15   \n",
       "0       U.S.S.R.-Cuba       single  mistral_7B        0.001          15   \n",
       "0      England-Jordan       single  mistral_7B        0.001           4   \n",
       "0       France-Israel       single  mistral_7B        0.001          10   \n",
       "0      U.S.A.-Germany       single  mistral_7B        0.001          12   \n",
       "0      U.S.S.R.-Syria       single  mistral_7B        0.001           5   \n",
       "0      France-Algeria       single  mistral_7B        0.001          10   \n",
       "\n",
       "  sim_score_2 sim_diff p-value  \n",
       "0          15       -3          \n",
       "0           0       10          \n",
       "0           0        0          \n",
       "0          18       -3          \n",
       "0           6        4          \n",
       "0          15        3          \n",
       "0           0        0          \n",
       "0           8        0          \n",
       "0          18       -3          \n",
       "0           0        6          \n",
       "0          18        0          \n",
       "0           0        0          \n",
       "0          15       -5          \n",
       "0          18       -3          \n",
       "0          10        5          \n",
       "0           0       15          \n",
       "0           0        4          \n",
       "0          12       -2          \n",
       "0          13       -1          \n",
       "0           0        5          \n",
       "0          15       -5          "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_path, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
